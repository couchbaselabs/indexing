Key-Value in separate file:
    Actually I am storing key, value and docid for each index entry and I am
doing that in a separate file. With 4K btree blocks, we can store around 256
entries per block. When the tree grows 4 level deep, it would mean an 
approximate 8*4 lookups for keys and 8*4 lookups for docid => 64 lookup
into the key-value file. This could translate into 16 block reads on SSDs with
4K block and SSDs could serve upto 70K random blocks reads. So we could do 4300
(70000/16) lookups into the index file. This seem to be the theoritical 
limitations if we store key, value in separate file.

Encoding and decoding btree blocks:
    Btree blocks are structured data containing 2 int64 fields and 3 int64
arrays, for keys, docids and values. This data should be encoded into sequence
of bytes before persisting on disk. Using Go's "encoding/binary" takes 1mS to
encode 64K btree block and using Go's "encoding/gob" takes around 500uS to
encode the same block. Every index lookup will force the algorithm to load the
leaf-block and hence 4300 access will lead to 1.8 Seconds of computation spent
on decoding/encoding.
    If we have to implement similar logic in C, is it not enough to just type
cast raw data into arrays of int64, provided endianness is taken care of ? Go
might give some thing similar through its "unsafe" package, but don't know
whether it is the right thing to do.

Garbage collection:
    We do copy-on-write to allow concurrent reads. This means for a btree with
size of 4K block and 4 level deep will have to copy 4 blocks for every insert.
And once outstanding reads are finished, the staleblocks generated by the
insert will have to be garbage collected. This translates to 806MB
(64K * 4 * 4300) of garbage collected every second, assuming there are 4300
inserts every second.
    I don't know about the internals of Garbage-Collection. I wonder wether
GC can keep up to this without hoging the system.
